TODO:
- check for urls that should not be added where the page is accessed in a round about way
- Detect redirects and if the page redirects your crawler, index the redirected content
- Detect and avoid dead URLs that return a 200 status but no data (click here to see what the different HTTP status codes meanLinks to an external site.)
- Detect and avoid crawling very large files, especially if they have low information value
- Detect and avoid sets of similar pages with no information
- ensure that you send the server a request with an ASCII URL
- write simple automatic trap detection systems based on repeated URL patterns and/or (ideally) webpage content similarity repetition over a certain amount of chained pages
- comment what code does for interview -- Ethan
- make code cleaner looking


extra credit:
- robot and sitemaps
- git

To-be-verified is Done:
- change tokens to just words 
- check for low information value and define it (do this by checking if unique word count is greater than n number of words)
    - Instead, used a threshold ratio of .1; calculated by unique words divided by total words
- alphabetically order subdomains

Done:
- typo in "reocurring" for "50 most reoccurring..."
- print results for 4 report analytics including token stuff
- check config
- format so that most of code is in_valid
- check only links with correct domains
- check for traps
- do not include fragment IDs
- transform relative to abs urls
- do http https check
- check re expression for tokenizer
- status issue with 604
- check for stop words