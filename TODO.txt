TODO:
- add http and https to any link, and make sure same link with different scheme is considered one link
- check for low information value and define it
- comment what code does for interview
- check for urls that should not be added where the page is accessed in a round about way
- Detect redirects and if the page redirects your crawler, index the redirected content
- Detect and avoid dead URLs that return a 200 status but no data (click here to see what the different HTTP status codes meanLinks to an external site.)
- Detect and avoid crawling very large files, especially if they have low information value
- Detect and avoid sets of similar pages with no information
- ensure that you send the server a request with an ASCII URL
- write simple automatic trap detection systems based on repeated URL patterns and/or (ideally) webpage content similarity repetition over a certain amount of chained pages
- print results for 4 report analytics including token stuff

extra credit:
- robot and sitemaps
- git


Done:
- check config
- format so that most of code is in_valid
- check only links with correct domains
- check for traps
- do not include fragment IDs
- transform relative to abs urls
- do http https check
- check re expression for tokenizer
- status issue with 604
- check for stop words